{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance predictor with simple NN\n",
    "\n",
    "A simple self-supervised setting to predict the number of moves it takes to reach final position\n",
    "\n",
    "This is a starting place to check things like state space representation (does using state = [1, 2, 3, 1] work), and just generally how well NNs can approximate permutation problems.\n",
    "\n",
    "### Basic outline:\n",
    "\n",
    "- Generate a set of `n'` moves by uniformly sampling from available actions, with `n'` randomly sampled from some probability distribution\n",
    "- Using greedy_reduce to simplify those moves to get `n` number of moves, $n \\leq n'$\n",
    "- Apply these `n` moves on a puzzle to reach `start_state`\n",
    "- Batch produce pairs of `x = start_state, y = n`\n",
    "- A network $\\mathcal{F}$ takes `start_state` as input, and the target output is `n'`\n",
    "\n",
    "### Some preliminary details\n",
    "\n",
    "- Loss function: mean square loss\n",
    "- Neural network weights: $|s| \\times 128$, $128 \\times 128$, $128 \\times 1$\n",
    "- For 2x2 puzzles, we only need up to 10 - 14 moves\n",
    "- State-space representation should be normalized?\n",
    "\n",
    "### Some hypotheses\n",
    "\n",
    "- Expecting better performance on low `n` over high `n`\n",
    "- Expecting weird things to happen at `n` > 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mechanism.permute import reverse_perm, permute_with_swap, perm_to_swap\n",
    "from src.mechanism.utils import get_inverse_move\n",
    "from src.mechanism.reduce import iterate_reduce_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a puzzle \n",
    "Since each puzzle is trained separately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_puzzle_moves(\n",
    "    puzzle_name: str, convert_to_swaps=True\n",
    ") -> (Dict[str, List[int]], int):\n",
    "    \"\"\"Retrieves and returns the moves and final position of the puzzle\"\"\"\n",
    "    # load the moves:\n",
    "    with open(f\"puzzles/{puzzle_name}/moves.json\") as f:\n",
    "        moves = json.load(f)\n",
    "\n",
    "    num_states = len(list(moves.values())[0])\n",
    "    # add reversed moves\n",
    "    reversed_moves = {}\n",
    "    for move_name, perm in moves.items():\n",
    "        reversed_perm = reverse_perm(perm)\n",
    "        if reversed_perm == perm:\n",
    "            continue\n",
    "        reversed_moves[f\"-{move_name}\"] = reversed_perm\n",
    "\n",
    "    moves.update(reversed_moves)\n",
    "\n",
    "    if convert_to_swaps:\n",
    "        for move_name, perm in moves.items():\n",
    "            moves[move_name] = perm_to_swap(perm)\n",
    "\n",
    "    # # get final position (from the first puzzle), note that the actual state of this position doesn't really matter\n",
    "    # # we just need to get the structure of the puzzle\n",
    "    # df = pd.read_csv(f'puzzles/{puzzle_name}/puzzles.csv')\n",
    "    # state = df.iloc[0].to_numpy()[3]\n",
    "\n",
    "    return moves, num_states\n",
    "\n",
    "\n",
    "puzzle_name = \"cube_3x3x3\"\n",
    "move_dict, num_states = load_puzzle_moves(puzzle_name)\n",
    "\n",
    "move_names = np.array(list(move_dict.keys()))\n",
    "\n",
    "final_state = list(range(num_states))\n",
    "\n",
    "print(f\"Loaded {puzzle_name} with {len(move_names)} moves and {num_states} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "The self-supervised part: Generate a set of `n` moves by uniformly sampling from available actions, with `n` randomly sampled from some probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_moves(move_names: List[str], n: int) -> List[int]:\n",
    "    return np.random.choice(move_names, n)\n",
    "\n",
    "\n",
    "def generate_state_from_moves(move_names, move_dict, state, inverse=False):\n",
    "    for move_name in move_names:\n",
    "        if inverse:\n",
    "            move_name = get_inverse_move(move_name)\n",
    "        move = move_dict[move_name]\n",
    "        state = permute_with_swap(state, move)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def normalize_state(state):\n",
    "    if type(state) == list:\n",
    "        return [s / len(state) for s in state]\n",
    "    return state / len(state)\n",
    "\n",
    "\n",
    "path = list(sample_moves(move_names, 15))\n",
    "\n",
    "print(path)\n",
    "\n",
    "path = iterate_reduce_sequence(path, puzzle_name)\n",
    "n = len(path)\n",
    "print(path)\n",
    "print(n)\n",
    "\n",
    "state = generate_state_from_moves(path, move_dict, final_state)\n",
    "\n",
    "normalize_state(torch.tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def sample(self) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Uniform_sampler(Sampler):\n",
    "    def __init__(self, low, high) -> None:\n",
    "        super().__init__()\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def sample(self) -> int:\n",
    "        return np.random.randint(self.low, self.high)\n",
    "\n",
    "\n",
    "class Constant_sampler(Sampler):\n",
    "    def __init__(self, n) -> None:\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def sample(self) -> int:\n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_sample(sampler):\n",
    "    # TODO: shouldn't use variables from outside scope like this\n",
    "    n = sampler.sample()\n",
    "    moves = list(sample_moves(move_names, n))\n",
    "    moves = iterate_reduce_sequence(moves, puzzle_name)\n",
    "    x = generate_state_from_moves(moves, move_dict, final_state)\n",
    "    x = normalize_state(x)\n",
    "    y = len(moves)\n",
    "\n",
    "    return torch.tensor(x), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(num_samples, sampler):\n",
    "    X = torch.empty(num_samples, num_states, dtype=torch.float32, requires_grad=False)\n",
    "    Y = torch.empty(num_samples, dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(num_samples):\n",
    "        x, y = generate_single_sample(sampler)\n",
    "        X[i, :] = x\n",
    "        Y[i] = y\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "sampler = Uniform_sampler(0, 1)\n",
    "X, Y = generate_batch(10, sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, inp: int, units: List[int]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = inp\n",
    "        for num_units in units:\n",
    "            layers.append(nn.Linear(prev, num_units))\n",
    "            layers.append(nn.BatchNorm1d(num_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = num_units\n",
    "\n",
    "        layers.append(nn.Linear(units[-1], 1))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "net = FFN(inp=num_states, units=[128, 128])\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 1\n",
    "batch_size = 1000\n",
    "\n",
    "net = FFN(inp=num_states, units=[64, 128, 128, 64])\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "sampler = Uniform_sampler(0, total_steps)\n",
    "# sampler = Constant_sampler(0)\n",
    "\n",
    "history = []\n",
    "net.train()\n",
    "for e in tqdm(range(500)):\n",
    "    X, Y = generate_batch(batch_size, sampler)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = net(X)\n",
    "    loss = criterion(y_pred, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Uniform_sampler(0, total_steps)\n",
    "# sampler = Constant_sampler(0)\n",
    "num_samples = 1000\n",
    "num_batches = 1\n",
    "\n",
    "total_errors = [0] * total_steps\n",
    "total_samples = [0] * total_steps\n",
    "total_preds = [0] * total_steps\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for b in range(num_batches):\n",
    "        X, Y = generate_batch(num_samples, sampler)\n",
    "        y_pred = net(X)\n",
    "\n",
    "        for y, y_p in zip(Y, y_pred):\n",
    "            total_errors[int(y)] += criterion(y_p, y)\n",
    "            total_samples[int(y)] += 1\n",
    "            total_preds[int(y)] += y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('number of samples for every n')\n",
    "plt.plot(total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Average error for every n\")\n",
    "plt.plot([e / n for (e, n) in zip(total_errors, total_samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Average prediction for every n\")\n",
    "plt.plot([y / n for (y, n) in zip(total_preds, total_samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
